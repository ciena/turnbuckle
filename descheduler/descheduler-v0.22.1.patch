# Copyright 2022 Ciena Corporation.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

diff --git a/.gitignore b/.gitignore
index 78fcbb93f..8de9e0fa3 100644
--- a/.gitignore
+++ b/.gitignore
@@ -4,4 +4,5 @@ vendordiff.patch
 .idea/
 *.code-workspace
 .vscode/
-kind
\ No newline at end of file
+kind
+*~
diff --git a/charts/descheduler/templates/clusterrole.yaml b/charts/descheduler/templates/clusterrole.yaml
index cf6946de8..2b27e5807 100644
--- a/charts/descheduler/templates/clusterrole.yaml
+++ b/charts/descheduler/templates/clusterrole.yaml
@@ -24,6 +24,11 @@ rules:
 - apiGroups: ["scheduling.k8s.io"]
   resources: ["priorityclasses"]
   verbs: ["get", "watch", "list"]
+{{- if .Values.clusterRole.rules }}
+{{- with .Values.clusterRole.rules }}
+{{- toYaml . | nindent 0 }}
+{{- end }}
+{{- end }}
 {{- if .Values.podSecurityPolicy.create }}
 - apiGroups: ['policy']
   resources: ['podsecuritypolicies']
diff --git a/charts/descheduler/templates/cronjob.yaml b/charts/descheduler/templates/cronjob.yaml
index 4ce2b4075..db52af621 100644
--- a/charts/descheduler/templates/cronjob.yaml
+++ b/charts/descheduler/templates/cronjob.yaml
@@ -59,6 +59,8 @@ spec:
               args:
                 - "--policy-config-file"
                 - "/policy-dir/policy.yaml"
+                - "--mitigation-grace-period"
+                - {{ .Values.mitigationGracePeriod | quote }}
                 {{- range $key, $value := .Values.cmdOptions }}
                 - {{ printf "--%s" $key | quote }}
                 {{- if $value }}
diff --git a/charts/descheduler/templates/deployment.yaml b/charts/descheduler/templates/deployment.yaml
index c0b60c2c4..6210d6e8f 100644
--- a/charts/descheduler/templates/deployment.yaml
+++ b/charts/descheduler/templates/deployment.yaml
@@ -38,6 +38,8 @@ spec:
             - "/policy-dir/policy.yaml"
             - "--descheduling-interval"
             - {{ required "deschedulingInterval required for running as Deployment" .Values.deschedulingInterval }}
+            - "--mitigation-grace-period"
+            - {{ .Values.mitigationGracePeriod | quote }}
             {{- range $key, $value := .Values.cmdOptions }}
             - {{ printf "--%s" $key | quote }}
             {{- if $value }}
diff --git a/charts/descheduler/values.yaml b/charts/descheduler/values.yaml
index d921248f3..5920f4bd2 100644
--- a/charts/descheduler/values.yaml
+++ b/charts/descheduler/values.yaml
@@ -25,13 +25,14 @@ nameOverride: ""
 fullnameOverride: ""
 
 cronJobApiVersion: "batch/v1"  # Use "batch/v1beta1" for k8s version < 1.21.0. TODO(@7i) remove with 1.23 release
-schedule: "*/2 * * * *"
+schedule: "*/1 * * * *"
 #startingDeadlineSeconds: 200
 #successfulJobsHistoryLimit: 1
 #failedJobsHistoryLimit: 1
 
 # Required when running as a Deployment
 deschedulingInterval: 5m
+mitigationGracePeriod: 1m
 
 cmdOptions:
   v: 3
@@ -41,29 +42,8 @@ cmdOptions:
 
 deschedulerPolicy:
   strategies:
-    RemoveDuplicates:
+    ConstraintPolicyEvaluation:
       enabled: true
-    RemovePodsViolatingNodeTaints:
-      enabled: true
-    RemovePodsViolatingNodeAffinity:
-      enabled: true
-      params:
-        nodeAffinityType:
-        - requiredDuringSchedulingIgnoredDuringExecution
-    RemovePodsViolatingInterPodAntiAffinity:
-      enabled: true
-    LowNodeUtilization:
-      enabled: true
-      params:
-        nodeResourceUtilizationThresholds:
-          thresholds:
-            cpu: 20
-            memory: 20
-            pods: 20
-          targetThresholds:
-            cpu: 50
-            memory: 50
-            pods: 50
 
 priorityClassName: system-cluster-critical
 
@@ -76,6 +56,16 @@ tolerations: []
 #   value: 'tool'
 #   effect: 'NoSchedule'
 
+clusterRole:
+  create: true
+  rules:
+    - apiGroups: [""]
+      resources: ["services"]
+      verbs: ["list"]
+    - apiGroups: ["constraint.ciena.com"]
+      resources: ["constraintpolicies", "constraintpolicyoffers", "constraintpolicybindings", "constraintpolicybindings/status"]
+      verbs: ["create", "update", "delete", "get", "watch", "list"]
+
 rbac:
   # Specifies whether RBAC resources should be created
   create: true
diff --git a/cmd/descheduler/app/options/options.go b/cmd/descheduler/app/options/options.go
index 2825a9367..c705f7801 100644
--- a/cmd/descheduler/app/options/options.go
+++ b/cmd/descheduler/app/options/options.go
@@ -19,11 +19,11 @@ package options
 
 import (
 	"github.com/spf13/pflag"
-
 	utilerrors "k8s.io/apimachinery/pkg/util/errors"
 	apiserveroptions "k8s.io/apiserver/pkg/server/options"
 	clientset "k8s.io/client-go/kubernetes"
 	"k8s.io/component-base/logs"
+	"time"
 
 	"sigs.k8s.io/descheduler/pkg/apis/componentconfig"
 	"sigs.k8s.io/descheduler/pkg/apis/componentconfig/v1alpha1"
@@ -31,7 +31,9 @@ import (
 )
 
 const (
-	DefaultDeschedulerPort = 10258
+	DefaultDeschedulerPort        = 10258
+	DefaultDeschedulingRunTimeout = 1 * time.Minute
+	DefaultMitigationGracePeriod  = 2 * time.Minute
 )
 
 // DeschedulerServer configuration
@@ -54,6 +56,9 @@ func NewDeschedulerServer() (*DeschedulerServer, error) {
 	secureServing := apiserveroptions.NewSecureServingOptions().WithLoopback()
 	secureServing.BindPort = DefaultDeschedulerPort
 
+	cfg.DeschedulingRunTimeout = DefaultDeschedulingRunTimeout
+	cfg.MitigationGracePeriod = DefaultMitigationGracePeriod
+
 	return &DeschedulerServer{
 		DeschedulerConfiguration: *cfg,
 		Logs:                     logs.NewOptions(),
@@ -82,6 +87,8 @@ func newDefaultComponentConfig() (*componentconfig.DeschedulerConfiguration, err
 func (rs *DeschedulerServer) AddFlags(fs *pflag.FlagSet) {
 	fs.StringVar(&rs.Logging.Format, "logging-format", "text", `Sets the log format. Permitted formats: "text", "json". Non-default formats don't honor these flags: --add-dir-header, --alsologtostderr, --log-backtrace-at, --log-dir, --log-file, --log-file-max-size, --logtostderr, --skip-headers, --skip-log-headers, --stderrthreshold, --log-flush-frequency.\nNon-default choices are currently alpha and subject to change without warning.`)
 	fs.DurationVar(&rs.DeschedulingInterval, "descheduling-interval", rs.DeschedulingInterval, "Time interval between two consecutive descheduler executions. Setting this value instructs the descheduler to run in a continuous loop at the interval specified.")
+	fs.DurationVar(&rs.DeschedulingRunTimeout, "descheduling-run-timeout", rs.DeschedulingRunTimeout, "Time to wait for all descheduling strategies to complete for each interval run.")
+	fs.DurationVar(&rs.MitigationGracePeriod, "mitigation-grace-period", rs.MitigationGracePeriod, "Time to wait after mitigation before pod is evicted.")
 	fs.StringVar(&rs.KubeconfigFile, "kubeconfig", rs.KubeconfigFile, "File with  kube configuration.")
 	fs.StringVar(&rs.PolicyConfigFile, "policy-config-file", rs.PolicyConfigFile, "File with descheduler policy configuration.")
 	fs.BoolVar(&rs.DryRun, "dry-run", rs.DryRun, "execute descheduler in dry run mode.")
diff --git a/deploy.sh b/deploy.sh
new file mode 100755
index 000000000..88044beb3
--- /dev/null
+++ b/deploy.sh
@@ -0,0 +1,56 @@
+#!/usr/bin/env bash
+
+OPTIND=1
+
+namespace="turnbuckle-system"
+tag=""
+repo="k8s.gcr.io/descheduler/descheduler"
+kubeconfig=$KUBECONFIG
+delete=0
+
+usage() {
+    echo "deploy.sh -n|namespace -t|image tag -r |image repo -k|kubeconfig -d|delete"
+    exit 0
+}
+
+while getopts "h?n:t:r:k:d" opt; do
+    case "$opt" in
+	h|\?)
+	    usage
+	    ;;
+	n)
+	    namespace=$OPTARG
+	    ;;
+	t)
+	    tag=$OPTARG
+	    ;;
+	r)
+	    repo=$OPTARG
+	    ;;
+	k)
+	    kubeconfig=$OPTARG
+	    ;;
+	d)
+	    delete=1
+	    ;;
+    esac
+done
+
+shift $((OPTIND-1))
+
+if [ $delete -eq 1 ]; then
+    echo "Deleting descheduler installation from namespace $namespace"
+    helm --kubeconfig $kubeconfig delete -n $namespace descheduler
+    exit $?
+fi
+
+echo "Installing descheduler with namespace $namespace, repo $repo, tag $tag"
+
+if [ "x$kubeconfig" = "x" ]; then
+    helm upgrade --install --wait --create-namespace --namespace $namespace --set image.tag=$tag --set image.repository=$repo --set image.pullPolicy=Always --values ./charts/descheduler/values.yaml descheduler ./charts/descheduler/
+else
+    echo "Using kubeconfig $kubeconfig"
+    helm --kubeconfig $kubeconfig upgrade --install --wait --create-namespace --namespace $namespace --set image.tag=$tag --set image.repository=$repo --set image.pullPolicy=Always --values ./charts/descheduler/values.yaml descheduler ./charts/descheduler/
+fi
+
+exit $?
diff --git a/go.mod b/go.mod
index e2d41dda0..deb44b378 100644
--- a/go.mod
+++ b/go.mod
@@ -3,17 +3,51 @@ module sigs.k8s.io/descheduler
 go 1.16
 
 require (
+	github.com/ciena/turnbuckle v0.0.3-alpha
 	github.com/client9/misspell v0.3.4
 	github.com/spf13/cobra v1.1.3
 	github.com/spf13/pflag v1.0.5
-	k8s.io/api v0.22.0
-	k8s.io/apimachinery v0.22.0
-	k8s.io/apiserver v0.22.0
-	k8s.io/client-go v0.22.0
-	k8s.io/code-generator v0.22.0
-	k8s.io/component-base v0.22.0
-	k8s.io/component-helpers v0.22.0
+	google.golang.org/grpc v1.40.0
+	k8s.io/api v0.22.3
+	k8s.io/apimachinery v0.22.3
+	k8s.io/apiserver v0.22.3
+	k8s.io/client-go v0.22.3
+	k8s.io/code-generator v0.22.3
+	k8s.io/component-base v0.22.3
+	k8s.io/component-helpers v0.22.3
 	k8s.io/klog/v2 v2.9.0
+	k8s.io/kube-scheduler v0.22.3 // indirect
 	k8s.io/kubectl v0.20.5
+	k8s.io/kubernetes v1.22.3 // indirect
+	sigs.k8s.io/controller-runtime v0.10.3
 	sigs.k8s.io/mdtoc v1.0.1
 )
+
+replace (
+	k8s.io/api => k8s.io/api v0.22.3
+	k8s.io/apiextensions-apiserver => k8s.io/apiextensions-apiserver v0.22.3
+	k8s.io/apimachinery => k8s.io/apimachinery v0.22.3
+	k8s.io/apiserver => k8s.io/apiserver v0.22.3
+	k8s.io/cli-runtime => k8s.io/cli-runtime v0.22.3
+	k8s.io/client-go => k8s.io/client-go v0.22.3
+	k8s.io/cloud-provider => k8s.io/cloud-provider v0.22.3
+	k8s.io/cluster-bootstrap => k8s.io/cluster-bootstrap v0.22.3
+	k8s.io/code-generator => k8s.io/code-generator v0.22.3
+	k8s.io/component-base => k8s.io/component-base v0.22.3
+	k8s.io/component-helpers => k8s.io/component-helpers v0.22.3
+	k8s.io/controller-manager => k8s.io/controller-manager v0.22.3
+	k8s.io/cri-api => k8s.io/cri-api v0.22.3
+	k8s.io/csi-translation-lib => k8s.io/csi-translation-lib v0.22.3
+	k8s.io/kube-aggregator => k8s.io/kube-aggregator v0.22.3
+	k8s.io/kube-controller-manager => k8s.io/kube-controller-manager v0.22.3
+	k8s.io/kube-proxy => k8s.io/kube-proxy v0.22.3
+	k8s.io/kube-scheduler => k8s.io/kube-scheduler v0.22.3
+	k8s.io/kubectl => k8s.io/kubectl v0.22.3
+	k8s.io/kubelet => k8s.io/kubelet v0.22.3
+	k8s.io/kubernetes => k8s.io/kubernetes v1.22.3
+	k8s.io/legacy-cloud-providers => k8s.io/legacy-cloud-providers v0.22.3
+	k8s.io/metrics => k8s.io/metrics v0.22.3
+	k8s.io/mount-utils => k8s.io/mount-utils v0.22.3
+	k8s.io/pod-security-admission => k8s.io/pod-security-admission v0.22.3
+	k8s.io/sample-apiserver => k8s.io/sample-apiserver v0.22.3
+)
diff --git a/pkg/apis/componentconfig/types.go b/pkg/apis/componentconfig/types.go
index d5aa919b9..725a7782d 100644
--- a/pkg/apis/componentconfig/types.go
+++ b/pkg/apis/componentconfig/types.go
@@ -31,6 +31,13 @@ type DeschedulerConfiguration struct {
 	// Time interval for descheduler to run
 	DeschedulingInterval time.Duration
 
+	// Time to wait for each descheduling run to complete
+	DeschedulingRunTimeout time.Duration
+
+	// Mitigation Grace period. A strategy option for connectpolicy
+	// Specifies the duration to wait after attempting mitigation before pod is evicted
+	MitigationGracePeriod time.Duration
+
 	// KubeconfigFile is path to kubeconfig file with authorization and master
 	// location information.
 	KubeconfigFile string
diff --git a/pkg/apis/componentconfig/v1alpha1/zz_generated.conversion.go b/pkg/apis/componentconfig/v1alpha1/zz_generated.conversion.go
index 8bb810eaa..9b9c308d3 100644
--- a/pkg/apis/componentconfig/v1alpha1/zz_generated.conversion.go
+++ b/pkg/apis/componentconfig/v1alpha1/zz_generated.conversion.go
@@ -1,3 +1,4 @@
+//go:build !ignore_autogenerated
 // +build !ignore_autogenerated
 
 /*
diff --git a/pkg/apis/componentconfig/v1alpha1/zz_generated.deepcopy.go b/pkg/apis/componentconfig/v1alpha1/zz_generated.deepcopy.go
index e5212d0a7..bd3a7b82a 100644
--- a/pkg/apis/componentconfig/v1alpha1/zz_generated.deepcopy.go
+++ b/pkg/apis/componentconfig/v1alpha1/zz_generated.deepcopy.go
@@ -1,3 +1,4 @@
+//go:build !ignore_autogenerated
 // +build !ignore_autogenerated
 
 /*
diff --git a/pkg/apis/componentconfig/v1alpha1/zz_generated.defaults.go b/pkg/apis/componentconfig/v1alpha1/zz_generated.defaults.go
index eed8e6d8c..106f23173 100644
--- a/pkg/apis/componentconfig/v1alpha1/zz_generated.defaults.go
+++ b/pkg/apis/componentconfig/v1alpha1/zz_generated.defaults.go
@@ -1,3 +1,4 @@
+//go:build !ignore_autogenerated
 // +build !ignore_autogenerated
 
 /*
diff --git a/pkg/apis/componentconfig/zz_generated.deepcopy.go b/pkg/apis/componentconfig/zz_generated.deepcopy.go
index a100965a1..701de74b9 100644
--- a/pkg/apis/componentconfig/zz_generated.deepcopy.go
+++ b/pkg/apis/componentconfig/zz_generated.deepcopy.go
@@ -1,3 +1,4 @@
+//go:build !ignore_autogenerated
 // +build !ignore_autogenerated
 
 /*
diff --git a/pkg/descheduler/descheduler.go b/pkg/descheduler/descheduler.go
index 445c6b2f2..70e4b24a1 100644
--- a/pkg/descheduler/descheduler.go
+++ b/pkg/descheduler/descheduler.go
@@ -18,8 +18,11 @@ package descheduler
 
 import (
 	"context"
+	"errors"
 	"fmt"
 	"sigs.k8s.io/descheduler/pkg/descheduler/strategies/nodeutilization"
+	"sync"
+	"time"
 
 	v1 "k8s.io/api/core/v1"
 	clientset "k8s.io/client-go/kubernetes"
@@ -64,7 +67,7 @@ func Run(rs *options.DeschedulerServer) error {
 	return RunDeschedulerStrategies(ctx, rs, deschedulerPolicy, evictionPolicyGroupVersion, stopChannel)
 }
 
-type strategyFunction func(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor)
+type strategyFunction func(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor, options ...strategies.StrategyOption)
 
 func RunDeschedulerStrategies(ctx context.Context, rs *options.DeschedulerServer, deschedulerPolicy *api.DeschedulerPolicy, evictionPolicyGroupVersion string, stopChannel chan struct{}) error {
 	sharedInformerFactory := informers.NewSharedInformerFactory(rs.Client, 0)
@@ -84,6 +87,7 @@ func RunDeschedulerStrategies(ctx context.Context, rs *options.DeschedulerServer
 		"PodLifeTime":                                 strategies.PodLifeTime,
 		"RemovePodsViolatingTopologySpreadConstraint": strategies.RemovePodsViolatingTopologySpreadConstraint,
 		"RemoveFailedPods":                            strategies.RemoveFailedPods,
+		"ConstraintPolicyEvaluation":                  strategies.ConstraintPolicyEvaluation,
 	}
 
 	nodeSelector := rs.NodeSelector
@@ -139,16 +143,42 @@ func RunDeschedulerStrategies(ctx context.Context, rs *options.DeschedulerServer
 			ignorePvcPods,
 		)
 
+		// It is possible that after a single call to the strategies the descheduer exits. This assumes that each
+		// strategy can complete its work in a single call. This may not be the case and some strategies may
+		// need to continue processing for a short while. To accommodate this a wait group option has been
+		// added. If a strategy needs to complete some tasks (such as updating a k8s resource which may
+		// take several retries because of object version clashes) they can add themselves to the wait group.
+		var wg sync.WaitGroup
+
 		for name, strategy := range deschedulerPolicy.Strategies {
 			if f, ok := strategyFuncs[name]; ok {
 				if strategy.Enabled {
-					f(ctx, rs.Client, strategy, nodes, podEvictor)
+					f(ctx, rs.Client, strategy, nodes, podEvictor, strategies.WithKubeconfigFile(rs.KubeconfigFile),
+						strategies.WithWaitGroup(&wg), strategies.WithMitigationGracePeriod(rs.MitigationGracePeriod))
 				}
 			} else {
 				klog.ErrorS(fmt.Errorf("unknown strategy name"), "skipping strategy", "strategy", name)
 			}
 		}
 
+		// Wait for all strategies on the wait group to complete
+		klog.V(1).InfoS("Waiting for strategies to complete...", "waitTime", rs.DeschedulingRunTimeout.String())
+
+		c := make(chan struct{})
+
+		go func() {
+			defer close(c)
+			wg.Wait()
+		}()
+
+		select {
+		case <-c:
+			klog.V(1).InfoS("All strategies run to completion")
+		case <-time.After(rs.DeschedulingRunTimeout):
+			// NOTE: This leaks a go routine until the original wait group completes
+			klog.V(0).ErrorS(errors.New("time-out"), "Timeout while waiting for strategies to complete", "timeout", rs.DeschedulingRunTimeout.String())
+		}
+
 		klog.V(1).InfoS("Number of evicted pods", "totalEvicted", podEvictor.TotalEvicted())
 
 		// If there was no interval specified, send a signal to the stopChannel to end the wait.Until loop after 1 iteration
diff --git a/pkg/descheduler/strategies/constraintpolicy.go b/pkg/descheduler/strategies/constraintpolicy.go
new file mode 100644
index 000000000..d6cbc1b5d
--- /dev/null
+++ b/pkg/descheduler/strategies/constraintpolicy.go
@@ -0,0 +1,623 @@
+package strategies
+
+import (
+	"context"
+	"errors"
+	"fmt"
+	"io/ioutil"
+	"math/rand"
+	"regexp"
+	"strings"
+	"sync"
+	"time"
+
+	constraintv1alpha1 "github.com/ciena/turnbuckle/pkg/apis/constraint/v1alpha1"
+	"github.com/ciena/turnbuckle/pkg/apis/underlay"
+	"google.golang.org/grpc"
+	v1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/runtime"
+	"k8s.io/apimachinery/pkg/runtime/schema"
+	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
+	clientset "k8s.io/client-go/kubernetes"
+	"k8s.io/client-go/rest"
+	"k8s.io/client-go/tools/clientcmd"
+	"k8s.io/client-go/util/workqueue"
+	klog "k8s.io/klog/v2"
+	ctlrclient "sigs.k8s.io/controller-runtime/pkg/client"
+	"sigs.k8s.io/descheduler/pkg/api"
+	"sigs.k8s.io/descheduler/pkg/descheduler/evictions"
+)
+
+var localSchemeBuilder = runtime.SchemeBuilder{
+	constraintv1alpha1.AddToScheme,
+}
+var Scheme = runtime.NewScheme()
+
+func init() {
+	// Seed the random number generator to each run
+	// will have a unique sequence
+	rand.Seed(time.Now().UTC().Unix())
+	metav1.AddToGroupVersion(Scheme, schema.GroupVersion{Version: "v1"})
+	utilruntime.Must(localSchemeBuilder.AddToScheme(Scheme))
+
+}
+
+type constraintPolicyBindingStatus struct {
+	name               string
+	namespace          string
+	mitigatedTimestamp metav1.Time
+}
+
+type podUpdateInfo struct {
+	pod       *v1.Pod
+	finalizer string
+}
+
+var errInvalidResource = errors.New("invalid-resource")
+var errNotFound = errors.New("not-found")
+var errCreateK8sClient = errors.New("create-k8s-client")
+var errPodEvictionFailure = errors.New("pod-eviction-failure")
+var updateItems []interface{}
+
+type UnderlayController interface {
+	Mitigate(pathId []string, src *v1.Node, peer *v1.Node, rules []*constraintv1alpha1.ConstraintPolicyRule) (string, error)
+	Release(string) error
+}
+
+type underlayController struct {
+	Service v1.Service
+}
+
+func getUnderlayController(client clientset.Interface) (UnderlayController, error) {
+	svcs, err := client.CoreV1().Services("").List(context.Background(),
+		metav1.ListOptions{LabelSelector: "constraint.ciena.com/underlay-controller"})
+	if err != nil {
+		return nil, err
+	}
+	if len(svcs.Items) == 0 {
+		return nil, errors.New("not-found")
+	}
+	return &underlayController{Service: svcs.Items[0]}, nil
+}
+
+func (c *underlayController) Release(pathId string) error {
+	var gopts []grpc.DialOption
+	dns := fmt.Sprintf("%s.%s.svc.cluster.local:9999", c.Service.Name, c.Service.Namespace)
+	gopts = append(gopts, grpc.WithInsecure())
+	conn, err := grpc.Dial(dns, gopts...)
+	if err != nil {
+		return err
+	}
+	defer conn.Close()
+
+	client := underlay.NewUnderlayControllerClient(conn)
+	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
+	defer cancel()
+
+	req := underlay.ReleaseRequest{Id: pathId}
+	_, err = client.Release(ctx, &req)
+	if err != nil {
+		return err
+	}
+	return nil
+}
+
+func (c *underlayController) Mitigate(currentPathId []string, src *v1.Node, peer *v1.Node, rules []*constraintv1alpha1.ConstraintPolicyRule) (string, error) {
+	var gopts []grpc.DialOption
+	dns := fmt.Sprintf("%s.%s.svc.cluster.local:9999", c.Service.Name, c.Service.Namespace)
+	gopts = append(gopts, grpc.WithInsecure())
+	conn, err := grpc.Dial(dns, gopts...)
+	if err != nil {
+		return "", err
+	}
+	defer conn.Close()
+
+	client := underlay.NewUnderlayControllerClient(conn)
+	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
+	defer cancel()
+
+	req := underlay.MitigateRequest{Id: currentPathId,
+		Src:  &underlay.NodeRef{Name: src.Name},
+		Peer: &underlay.NodeRef{Name: peer.Name},
+	}
+	resp, err := client.Mitigate(ctx, &req)
+	if err != nil {
+		return "", err
+	}
+	return resp.Id, nil
+}
+
+func getUnderlayPath(finalizers []string) []string {
+	paths := []string{}
+	finalizerPrefix := "constraint.ciena.io/remove-underlay_"
+	for _, finalizer := range finalizers {
+		if strings.HasPrefix(finalizer, finalizerPrefix) {
+			paths = append(paths, finalizer[len(finalizerPrefix):])
+		}
+	}
+	return paths
+}
+
+func belongsToList(val string, list []string) bool {
+	for _, l := range list {
+		if l == val {
+			return true
+		}
+	}
+	return false
+}
+
+func mitigate(client clientset.Interface, controller UnderlayController, srcPod *v1.Pod, dstPod *v1.Pod,
+	srcNode *v1.Node, dstNode *v1.Node, rules []*constraintv1alpha1.ConstraintPolicyRule) error {
+	paths := []string{}
+	if srcPod.GetFinalizers() != nil {
+		paths = getUnderlayPath(srcPod.GetFinalizers())
+	} else if dstPod.GetFinalizers() != nil {
+		paths = getUnderlayPath(dstPod.GetFinalizers())
+	}
+	klog.V(0).InfoS("descheduler-mitigate-with-paths", "path", paths)
+	pathId, err := controller.Mitigate(paths, srcNode, dstNode, rules)
+	if err != nil {
+		klog.V(0).ErrorS(err, "mitigate-failed", "src", srcNode.Name, "dst", dstNode.Name)
+		return err
+	}
+	klog.V(0).InfoS("descheduler-mitigate", "pathId", pathId, "src", srcNode.Name, "peer", dstNode.Name)
+	//update the src and dst pod with the path finalizer
+	finalizer := "constraint.ciena.io/remove-underlay_" + pathId
+	if !belongsToList(finalizer, srcPod.ObjectMeta.Finalizers) {
+		srcPod.ObjectMeta.Finalizers = append(srcPod.ObjectMeta.Finalizers, finalizer)
+		if _, err := client.CoreV1().Pods(srcPod.Namespace).Update(context.Background(), srcPod, metav1.UpdateOptions{}); err != nil {
+			klog.V(0).ErrorS(err, "mitigate-pod-finalizer-update-failed", "pod", srcPod.Name)
+			//enqueue for retry
+			klog.V(0).InfoS("mitigate-pod-update-enqueue", "pod", srcPod.Name)
+			updateItems = append(updateItems, &podUpdateInfo{pod: srcPod, finalizer: finalizer})
+		}
+	}
+	if !belongsToList(finalizer, dstPod.ObjectMeta.Finalizers) {
+		dstPod.ObjectMeta.Finalizers = append(dstPod.ObjectMeta.Finalizers, finalizer)
+		if _, err := client.CoreV1().Pods(dstPod.Namespace).Update(context.Background(), dstPod, metav1.UpdateOptions{}); err != nil {
+			klog.V(0).ErrorS(err, "mitigate-pod-finalizer-update-failed", "pod", dstPod.Name)
+			//enqueue for retry
+			klog.V(0).InfoS("mitigate-pod-update-enqueue", "pod", dstPod.Name)
+			updateItems = append(updateItems, &podUpdateInfo{pod: dstPod, finalizer: finalizer})
+		}
+	}
+	return nil
+}
+
+func getPodsAndNodesFromBinding(client clientset.Interface, binding *constraintv1alpha1.ConstraintPolicyBinding) (map[string]*v1.Pod, map[string]*v1.Node, error) {
+	if len(binding.Spec.Targets) == 0 {
+		return nil, nil, fmt.Errorf("no-targets-found-for-binding-%s", binding.GetName())
+	}
+
+	targets := make(map[string]*v1.Pod, len(binding.Spec.Targets))
+	podToNodeMap := make(map[string]*v1.Node, len(binding.Spec.Targets))
+
+	for name, ref := range binding.Spec.Targets {
+		if ref.Kind == "Pod" {
+			pod, err := client.CoreV1().Pods(ref.Namespace).Get(context.Background(), ref.Name, metav1.GetOptions{})
+			if err == nil {
+				node, err := client.CoreV1().Nodes().Get(context.Background(), pod.Spec.NodeName, metav1.GetOptions{})
+				if err == nil {
+					targets[name] = pod
+					podToNodeMap[pod.Name] = node
+				}
+			}
+		}
+	}
+
+	return targets, podToNodeMap, nil
+}
+
+func getPolicyRules(genericClient ctlrclient.Client, offer *constraintv1alpha1.ConstraintPolicyOffer) ([]*constraintv1alpha1.ConstraintPolicyRule, error) {
+	var rules []*constraintv1alpha1.ConstraintPolicyRule
+	for _, policyName := range offer.Spec.Policies {
+		var policy constraintv1alpha1.ConstraintPolicy
+		err := genericClient.Get(context.Background(),
+			ctlrclient.ObjectKey{
+				Namespace: offer.GetNamespace(),
+				Name:      string(policyName),
+			},
+			&policy)
+		if err != nil {
+			klog.V(0).ErrorS(err, "unknown-policy", "name", string(policyName))
+			continue
+		}
+		if rules == nil {
+			rules = policy.Spec.Rules
+		} else {
+			rules = mergeRules(rules, policy.Spec.Rules)
+		}
+	}
+	if len(rules) == 0 {
+		klog.V(0).InfoS("no-rules", "offer", offer.GetName())
+		return nil, fmt.Errorf("no-rules-found-for-offer-%s", offer.GetName())
+	}
+	return rules, nil
+}
+
+func updateBindingStatusTimestamp(genericClient ctlrclient.Client, binding *constraintv1alpha1.ConstraintPolicyBinding,
+	mitigatedTimestamp metav1.Time, enqueueOnFailure bool) error {
+	binding.Status.LastMitigatedTimestamp = mitigatedTimestamp
+	if err := genericClient.Status().Update(context.Background(),
+		binding); err != nil {
+		klog.V(0).ErrorS(err, "update-binding-status-failed", "binding", binding.GetName())
+		if enqueueOnFailure {
+			//enqueue to retry later
+			klog.V(0).InfoS("update-binding-status-enqueue", "binding", binding.GetName())
+			updateItems = append(updateItems, &constraintPolicyBindingStatus{
+				name: binding.GetName(), namespace: binding.GetNamespace(), mitigatedTimestamp: mitigatedTimestamp,
+			})
+		} else {
+			return err
+		}
+	}
+	return nil
+}
+
+func mergeRules(existingRules []*constraintv1alpha1.ConstraintPolicyRule, newRules []*constraintv1alpha1.ConstraintPolicyRule) []*constraintv1alpha1.ConstraintPolicyRule {
+	presenceMap := make(map[string]struct{})
+	for _, r := range existingRules {
+		presenceMap[r.Name] = struct{}{}
+	}
+	for _, r := range newRules {
+		if _, ok := presenceMap[r.Name]; !ok {
+			existingRules = append(existingRules, r)
+		}
+	}
+	return existingRules
+}
+
+func ConstraintPolicyEvaluation(
+	ctx context.Context,
+	client clientset.Interface,
+	strategy api.DeschedulerStrategy,
+	nodes []*v1.Node,
+	podEvictor *evictions.PodEvictor,
+	sopts ...StrategyOption) {
+
+	opts := buildStrategyOptions(sopts)
+
+	config, err := rest.InClusterConfig()
+	if err != nil {
+		if opts.kubeconfigFile == "" {
+			klog.V(0).ErrorS(errCreateK8sClient, "unable to create Kubernetes API client")
+			return
+		} else {
+			bytes, err := ioutil.ReadFile(opts.kubeconfigFile)
+			if err != nil {
+				klog.V(0).ErrorS(err, "unable to read config file", "filename", opts.kubeconfigFile)
+				return
+			}
+			config, err = clientcmd.RESTConfigFromKubeConfig(bytes)
+			if err != nil {
+				klog.V(0).ErrorS(err, "unable to create Kubernetes API client config")
+				return
+			}
+		}
+	}
+	genericClient, err := ctlrclient.New(config, ctlrclient.Options{
+		Scheme: Scheme,
+	})
+	if err != nil {
+		klog.V(0).ErrorS(err, "unable to create generic controller client")
+		return
+	}
+	uc, err := getUnderlayController(client)
+	if err != nil {
+		klog.V(0).ErrorS(err, "underlay-controller-not-found. Mitigation would be disabled")
+	}
+
+	var bindings constraintv1alpha1.ConstraintPolicyBindingList
+	err = genericClient.List(context.Background(), &bindings,
+		ctlrclient.InNamespace(metav1.NamespaceAll))
+	if err != nil {
+		klog.V(0).ErrorS(err, "unable to list constraint policy bindings")
+		return
+	}
+	if opts.mitigationGracePeriod.Seconds() == 0 {
+		opts.mitigationGracePeriod = time.Minute
+	}
+	klog.V(0).InfoS("descheduler", "mitigation-grace-period", opts.mitigationGracePeriod, "bindings", len(bindings.Items))
+	for _, binding := range bindings.Items {
+		if binding.GetDeletionTimestamp() != nil {
+			// being deleted, ignore
+			continue
+		}
+		comp := binding.Status.Compliance
+		klog.V(0).InfoS("compliance-evaluation",
+			"cluster", binding.GetClusterName(),
+			"namespace", binding.GetNamespace(),
+			"name", binding.GetName(),
+			"compliance", comp)
+		if comp == "Violation" {
+			var offer constraintv1alpha1.ConstraintPolicyOffer
+			err = genericClient.Get(context.Background(),
+				ctlrclient.ObjectKey{
+					Namespace: binding.GetNamespace(),
+					Name:      binding.Spec.Offer,
+				},
+				&offer)
+			if err != nil {
+				klog.V(0).ErrorS(err, "unknown-policy-offer", "name", binding.Spec.Offer)
+				continue
+			}
+			// Get violation policy
+			vpolicy := offer.Spec.ViolationPolicy
+			klog.V(0).InfoS("violation-policy", "policy", vpolicy)
+			if vpolicy == "Evict" {
+				targets, podToNodeMap, err := getPodsAndNodesFromBinding(client, binding)
+				if err != nil {
+					klog.V(0).ErrorS(err, "error-processing-binding-targets", "binding", binding.GetName())
+					continue
+				}
+
+				if len(targets) == 0 {
+					klog.V(0).InfoS("no-targets-to-be-processed", "binding", binding.GetName())
+					continue
+				}
+
+				var srcPod, dstPod *v1.Pod
+
+				if _, ok := targets["source"]; ok {
+					srcPod = targets["source"]
+				}
+
+				if _, ok := targets["destination"]; ok {
+					dstPod = targets["destination"]
+				}
+
+				var graceDuration time.Duration
+				if d, err := time.ParseDuration(offer.Spec.Grace); err == nil {
+					graceDuration = d
+				}
+				lastComplianceChangeTimestamp := binding.Status.LastComplianceChangeTimestamp
+				expiry := lastComplianceChangeTimestamp.Add(graceDuration)
+				if time.Now().Before(expiry) {
+					klog.V(0).InfoS("descheduler-eviction-grace-period", "binding", binding.GetName(), "offer", offer.GetName())
+					continue
+				}
+				lastMitigatedTimestamp := binding.Status.LastMitigatedTimestamp
+				if lastMitigatedTimestamp.IsZero() {
+					if uc != nil && srcPod != nil && dstPod != nil {
+						// get the offer policies
+						var policyRules []*constraintv1alpha1.ConstraintPolicyRule
+						rules, err := getPolicyRules(genericClient, &offer)
+						if err != nil {
+							klog.V(0).ErrorS(err, "rule-not-found", "offer", offer.GetName())
+						} else {
+							policyRules = rules
+						}
+						if err := mitigate(client, uc, srcPod, dstPod, podToNodeMap[srcPod.Name], podToNodeMap[dstPod.Name], policyRules); err == nil {
+							if err := updateBindingStatusTimestamp(genericClient, binding, metav1.Now(), true); err != nil {
+								klog.V(0).ErrorS(err, "update-binding-failed", "binding", binding.GetName(), "offer", offer.GetName())
+							} else {
+								klog.V(0).InfoS("update-binding-success", "binding", binding.GetName(), "offer", offer.GetName())
+							}
+							klog.V(0).InfoS("descheduler-mitigate", "src", srcPod.Name, "dst", dstPod.Name, "binding", binding.GetName())
+							continue
+						}
+					}
+				} else {
+					expiry := lastMitigatedTimestamp.Add(opts.mitigationGracePeriod)
+					if time.Now().Before(expiry) {
+						klog.V(0).InfoS("binding-under-mitigation-grace-period", "binding", binding.GetName())
+						continue
+					} else {
+						klog.V(0).InfoS("binding-outside-mitigation-grace-period", "binding", binding.GetName())
+					}
+				}
+				var targetPod *v1.Pod
+				var targetNode *v1.Node
+				// we are here when we dont have source/destination targets
+				// or when mitigation grace period has expired or mitigation failed for source/destination pods
+				switch {
+				case srcPod != nil && dstPod != nil:
+					klog.V(0).InfoS("selecting-from-src-dst-pods-to-evict", "src", srcPod.Name, "dst", dstPod.Name)
+					targetPods := []*v1.Pod{srcPod, dstPod}
+					targetNodes := []*v1.Node{podToNodeMap[srcPod.Name], podToNodeMap[dstPod.Name]}
+					target := rand.Intn(2)
+					targetPod, targetNode = targetPods[target], targetNodes[target]
+				default:
+					// randomly pick a pod from the targets
+					klog.V(0).InfoS("selecting-from-available-pods-to-evict", "num-pods", len(targets))
+					pods := make([]*v1.Pod, 0, len(targets))
+					for _, p := range targets {
+						pods = append(pods, p)
+					}
+					targetPod = pods[rand.Intn(len(pods))]
+					targetNode = podToNodeMap[targetPod.Name]
+				}
+				klog.V(0).InfoS("evict-pod", "namespace", targetPod.Namespace,
+					"name", targetPod.Name, "node", targetNode.Name)
+				ok, err := podEvictor.EvictPod(context.TODO(), targetPod, targetNode,
+					fmt.Sprintf("policy offer '%s' is in violation", offer.GetName()))
+				if err != nil {
+					klog.V(0).ErrorS(err, "pod-eviction-error",
+						"namespace", targetPod.Namespace, "name", targetPod.Name, "node", targetNode.Name)
+				} else if !ok {
+					klog.V(0).ErrorS(errPodEvictionFailure,
+						"namespace", targetPod.Namespace, "name", targetPod.Name, "node", targetNode.Name)
+				} else {
+					klog.V(0).InfoS("pod-evicted", "namespace", targetPod.Namespace,
+						"name", targetPod.Name, "node", targetNode.Name)
+				}
+			}
+		}
+	}
+
+	if len(updateItems) > 0 {
+		if opts.wg == nil {
+			klog.V(0).InfoS("no-waitgroup-created-to-update-batch. Call strategy WithWaitGroup option")
+		} else {
+			klog.V(0).InfoS("update-workqueue", "num-items", len(updateItems))
+			opts.wg.Add(1)
+			go updateWorkqueue(genericClient, client, updateItems, opts.wg)
+		}
+	}
+}
+
+func processBindingUpdate(genericClient ctlrclient.Client, constraintBinding *constraintPolicyBindingStatus) (requeue bool) {
+	var binding constraintv1alpha1.ConstraintPolicyBinding
+	requeue = false
+	err := genericClient.Get(context.Background(),
+		ctlrclient.ObjectKey{
+			Namespace: constraintBinding.namespace,
+			Name:      constraintBinding.name,
+		},
+		&binding)
+	if err != nil {
+		klog.V(0).ErrorS(err, "process-binding-update-failed", "binding", constraintBinding.name)
+		return
+	}
+	if err := updateBindingStatusTimestamp(genericClient, &binding, constraintBinding.mitigatedTimestamp, false); err != nil {
+		requeue = true
+	} else {
+		klog.V(0).InfoS("process-binding-update-success", "binding", constraintBinding.name)
+	}
+	return
+}
+
+func processPodUpdate(client clientset.Interface, podUpdate *podUpdateInfo) (requeue bool) {
+	requeue = false
+	//get the new copy of the pod
+	pod, err := client.CoreV1().Pods(podUpdate.pod.GetNamespace()).Get(context.Background(), podUpdate.pod.GetName(), metav1.GetOptions{})
+	if err != nil {
+		klog.V(0).ErrorS(err, "pod-update-get-failed", "pod", podUpdate.pod.GetName())
+		return
+	}
+	// check if the finalizer already exist.
+	finalizers := pod.GetFinalizers()
+	if finalizers != nil && belongsToList(podUpdate.finalizer, finalizers) {
+		//already exist.
+		return
+	}
+	//set the finalizer
+	pod.ObjectMeta.Finalizers = append(pod.ObjectMeta.Finalizers, podUpdate.finalizer)
+	if _, err := client.CoreV1().Pods(pod.GetNamespace()).Update(context.Background(), pod, metav1.UpdateOptions{}); err != nil {
+		klog.V(0).ErrorS(err, "pod-update-finalizer-failed", "pod", pod.GetName())
+		requeue = true
+	} else {
+		klog.V(0).InfoS("pod-update-success", "pod", pod.GetName())
+	}
+	return
+}
+
+func updateWorkqueue(genericClient ctlrclient.Client, client clientset.Interface, updateItems []interface{}, wg *sync.WaitGroup) {
+	defer wg.Done()
+	queue := workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter())
+	defer queue.ShutDown()
+	maxRequeues := 2
+	for len(updateItems) > 0 {
+		updateItem := updateItems[0]
+		updateItems = updateItems[1:]
+		klog.V(0).InfoS("update-workqueue-adding-item-ratelimited")
+		queue.AddRateLimited(updateItem)
+		// wait for the item to appear
+		item, quit := queue.Get()
+		if quit {
+			break
+		}
+
+		// nolint S1034
+		switch item.(type) {
+		case *constraintPolicyBindingStatus:
+			if requeue := processBindingUpdate(genericClient, item.(*constraintPolicyBindingStatus)); requeue {
+				// we don't want to requeue forever as this will freeze the descheduler on the waitgroup
+				// we allow up to 2 requeues
+				if queue.NumRequeues(item) <= maxRequeues {
+					updateItems = append(updateItems, item)
+					klog.V(0).InfoS("update-workqueue-enqueueing-binding-update-again", "numrequeues", queue.NumRequeues(item))
+				} else {
+					klog.V(0).InfoS("update-workqueue-done-retrying-binding-update", "numrequeues", queue.NumRequeues(item))
+					queue.Forget(item)
+				}
+			} else {
+				klog.V(0).InfoS("update-workqueue-binding-done")
+				queue.Forget(item)
+			}
+		case *podUpdateInfo:
+			if requeue := processPodUpdate(client, item.(*podUpdateInfo)); requeue {
+				if queue.NumRequeues(item) <= maxRequeues {
+					updateItems = append(updateItems, item)
+					klog.V(0).InfoS("update-workqueue-enqueueing-pod-update-again", "numrequeues", queue.NumRequeues(item))
+				} else {
+					klog.V(0).InfoS("update-workqueue-done-retrying-pod-update", "numrequeues", queue.NumRequeues(item))
+					queue.Forget(item)
+				}
+			} else {
+				klog.V(0).InfoS("update-workqueue-pod-done")
+				queue.Forget(item)
+			}
+		default:
+			klog.V(0).InfoS("update-workqueue-unknown-type")
+			queue.Forget(item)
+		}
+		queue.Done(item)
+	}
+}
+
+// The below is copied from the constraint-policy repository utilizing the go
+// proverb "A little copying is better than a little dependency". No need
+// to pull in the whole repo as a dependency for this little type.
+
+// Endpoint a concreate end point reference
+type Endpoint struct {
+	Cluster   string
+	Namespace string
+	Kind      string
+	Name      string
+	IP        string
+}
+
+func (ep Endpoint) String() string {
+	var buf strings.Builder
+
+	if ep.Cluster != "" {
+		buf.WriteString(ep.Cluster)
+		buf.WriteString("/")
+	}
+
+	if ep.Namespace != "" {
+		buf.WriteString(ep.Namespace)
+		buf.WriteString(":")
+	} else if ep.Cluster != "" {
+		buf.WriteString("default:")
+	}
+	if ep.Kind != "" {
+		buf.WriteString(ep.Kind)
+		buf.WriteString("/")
+	}
+	buf.WriteString(ep.Name)
+	if ep.IP != "" {
+		buf.WriteString("[")
+		buf.WriteString(ep.IP)
+		buf.WriteString("]")
+	}
+
+	return buf.String()
+}
+
+var endpointRE = regexp.MustCompile(`^((([a-zA-Z0-9_-]*)/)?([a-zA-Z0-9-]*):)?(([a-zA-Z0-9_-]*)/)?([a-zA-Z0-9_-]+)(\[([0-9.]*)\])?$`)
+
+// ParseEndpoint parses a string representation of an endpoint
+// to a Endpoint
+func ParseEndpoint(in string) (*Endpoint, error) {
+	var ep Endpoint
+
+	parts := endpointRE.FindStringSubmatch(in)
+
+	//fmt.Printf("%+#v\n", parts)
+	if len(parts) == 0 {
+		return nil, fmt.Errorf(`invalid endpoint "%s"`, in)
+	}
+	if len(parts) > 0 {
+		ep.Cluster = parts[3]
+		ep.Namespace = parts[4]
+		ep.Kind = parts[6]
+		ep.Name = parts[7]
+		ep.IP = parts[9]
+	}
+	return &ep, nil
+}
diff --git a/pkg/descheduler/strategies/duplicates.go b/pkg/descheduler/strategies/duplicates.go
index fe6169b0a..3b21db980 100644
--- a/pkg/descheduler/strategies/duplicates.go
+++ b/pkg/descheduler/strategies/duplicates.go
@@ -66,6 +66,7 @@ func RemoveDuplicatePods(
 	strategy api.DeschedulerStrategy,
 	nodes []*v1.Node,
 	podEvictor *evictions.PodEvictor,
+	opts ...StrategyOption,
 ) {
 	if err := validateRemoveDuplicatePodsParams(strategy.Params); err != nil {
 		klog.ErrorS(err, "Invalid RemoveDuplicatePods parameters")
diff --git a/pkg/descheduler/strategies/failedpods.go b/pkg/descheduler/strategies/failedpods.go
index 74d79956c..245d6bdb9 100644
--- a/pkg/descheduler/strategies/failedpods.go
+++ b/pkg/descheduler/strategies/failedpods.go
@@ -33,6 +33,7 @@ func RemoveFailedPods(
 	strategy api.DeschedulerStrategy,
 	nodes []*v1.Node,
 	podEvictor *evictions.PodEvictor,
+	opts ...StrategyOption,
 ) {
 	strategyParams, err := validateAndParseRemoveFailedPodsParams(ctx, client, strategy.Params)
 	if err != nil {
diff --git a/pkg/descheduler/strategies/node_affinity.go b/pkg/descheduler/strategies/node_affinity.go
index cd21b8ce3..273251e41 100644
--- a/pkg/descheduler/strategies/node_affinity.go
+++ b/pkg/descheduler/strategies/node_affinity.go
@@ -47,7 +47,7 @@ func validatePodsViolatingNodeAffinityParams(params *api.StrategyParameters) err
 }
 
 // RemovePodsViolatingNodeAffinity evicts pods on nodes which violate node affinity
-func RemovePodsViolatingNodeAffinity(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor) {
+func RemovePodsViolatingNodeAffinity(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor, opts ...StrategyOption) {
 	if err := validatePodsViolatingNodeAffinityParams(strategy.Params); err != nil {
 		klog.ErrorS(err, "Invalid RemovePodsViolatingNodeAffinity parameters")
 		return
diff --git a/pkg/descheduler/strategies/node_taint.go b/pkg/descheduler/strategies/node_taint.go
index f2310b514..1bca9b210 100644
--- a/pkg/descheduler/strategies/node_taint.go
+++ b/pkg/descheduler/strategies/node_taint.go
@@ -48,7 +48,7 @@ func validateRemovePodsViolatingNodeTaintsParams(params *api.StrategyParameters)
 }
 
 // RemovePodsViolatingNodeTaints evicts pods on the node which violate NoSchedule Taints on nodes
-func RemovePodsViolatingNodeTaints(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor) {
+func RemovePodsViolatingNodeTaints(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor, opts ...StrategyOption) {
 	if err := validateRemovePodsViolatingNodeTaintsParams(strategy.Params); err != nil {
 		klog.ErrorS(err, "Invalid RemovePodsViolatingNodeTaints parameters")
 		return
diff --git a/pkg/descheduler/strategies/nodeutilization/highnodeutilization.go b/pkg/descheduler/strategies/nodeutilization/highnodeutilization.go
index 467889963..c1e4caf7c 100644
--- a/pkg/descheduler/strategies/nodeutilization/highnodeutilization.go
+++ b/pkg/descheduler/strategies/nodeutilization/highnodeutilization.go
@@ -27,12 +27,13 @@ import (
 	"sigs.k8s.io/descheduler/pkg/api"
 	"sigs.k8s.io/descheduler/pkg/descheduler/evictions"
 	nodeutil "sigs.k8s.io/descheduler/pkg/descheduler/node"
+	"sigs.k8s.io/descheduler/pkg/descheduler/strategies"
 	"sigs.k8s.io/descheduler/pkg/utils"
 )
 
 // HighNodeUtilization evicts pods from under utilized nodes so that scheduler can schedule according to its strategy.
 // Note that CPU/Memory requests are used to calculate nodes' utilization and not the actual resource usage.
-func HighNodeUtilization(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor) {
+func HighNodeUtilization(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor, opts ...strategies.StrategyOption) {
 	if err := validateNodeUtilizationParams(strategy.Params); err != nil {
 		klog.ErrorS(err, "Invalid HighNodeUtilization parameters")
 		return
diff --git a/pkg/descheduler/strategies/nodeutilization/lownodeutilization.go b/pkg/descheduler/strategies/nodeutilization/lownodeutilization.go
index 70bfe6257..ce563a2c4 100644
--- a/pkg/descheduler/strategies/nodeutilization/lownodeutilization.go
+++ b/pkg/descheduler/strategies/nodeutilization/lownodeutilization.go
@@ -28,12 +28,13 @@ import (
 	"sigs.k8s.io/descheduler/pkg/api"
 	"sigs.k8s.io/descheduler/pkg/descheduler/evictions"
 	nodeutil "sigs.k8s.io/descheduler/pkg/descheduler/node"
+	"sigs.k8s.io/descheduler/pkg/descheduler/strategies"
 	"sigs.k8s.io/descheduler/pkg/utils"
 )
 
 // LowNodeUtilization evicts pods from overutilized nodes to underutilized nodes. Note that CPU/Memory requests are used
 // to calculate nodes' utilization and not the actual resource usage.
-func LowNodeUtilization(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor) {
+func LowNodeUtilization(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor, opts ...strategies.StrategyOption) {
 	// TODO: May be create a struct for the strategy as well, so that we don't have to pass along the all the params?
 	if err := validateNodeUtilizationParams(strategy.Params); err != nil {
 		klog.ErrorS(err, "Invalid LowNodeUtilization parameters")
diff --git a/pkg/descheduler/strategies/options.go b/pkg/descheduler/strategies/options.go
new file mode 100644
index 000000000..5b33f7fdb
--- /dev/null
+++ b/pkg/descheduler/strategies/options.go
@@ -0,0 +1,91 @@
+/*
+Copyright 2021 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+package strategies
+
+import (
+	"sync"
+	"time"
+)
+
+// StrategyOption represents an optional paramater to a strategy evaluation
+// function. The structure of the stratey option  is loosely based on
+// (blatently plagerized from) the GRPC client option code.
+type StrategyOption interface {
+	apply(*strategyOption)
+}
+
+// WithWaitGroup provides a strategy with a wait group reference that
+// can be utilized by the strategy to informm the calling loop when
+// the strategy is run to completion. This an be usedful if the
+// strategy needs to implement a retry loop for updating k8s resources.
+func WithWaitGroup(wg *sync.WaitGroup) StrategyOption {
+	return newFuncStrategyOption(func(o *strategyOption) {
+		o.wg = wg
+	})
+}
+
+// WithKubeconfigFile provides a strategy to a reference to a local
+// kube config file if that was set as a command line option
+func WithKubeconfigFile(f string) StrategyOption {
+	return newFuncStrategyOption(func(o *strategyOption) {
+		o.kubeconfigFile = f
+	})
+}
+
+// WithMitigationGracePeriod provides a strategy with a mitigation grace period
+// that can be utilized by the strategy to wait for a grace period after
+// attemping mitigation and before pod evictions
+
+func WithMitigationGracePeriod(d time.Duration) StrategyOption {
+	return newFuncStrategyOption(func(o *strategyOption) {
+		o.mitigationGracePeriod = d
+	})
+}
+
+// buildStrategyOptions combines the given options into a single
+// option structure
+func buildStrategyOptions(sopts []StrategyOption) *strategyOption {
+	opts := strategyOption{}
+	for _, sopt := range sopts {
+		sopt.apply(&opts)
+	}
+	return &opts
+}
+
+// strategyOption optional parameters to strategy evaluators. Each
+// strategy implementation can choose to leverage or ignore these
+// values and thus this can be used to add additional parameters
+// to strategies without having to update each evaluator signature.
+type strategyOption struct {
+	wg                    *sync.WaitGroup
+	kubeconfigFile        string
+	mitigationGracePeriod time.Duration
+}
+
+// funcStrategyOption wraps a function that modifies the
+// strategyOptions into an implementation of a StrategyOption
+// interface
+type funcStrategyOption struct {
+	f func(*strategyOption)
+}
+
+func (fso *funcStrategyOption) apply(o *strategyOption) {
+	fso.f(o)
+}
+
+func newFuncStrategyOption(f func(o *strategyOption)) *funcStrategyOption {
+	return &funcStrategyOption{f: f}
+}
diff --git a/pkg/descheduler/strategies/pod_antiaffinity.go b/pkg/descheduler/strategies/pod_antiaffinity.go
index 6fe9d5a29..9051b36ce 100644
--- a/pkg/descheduler/strategies/pod_antiaffinity.go
+++ b/pkg/descheduler/strategies/pod_antiaffinity.go
@@ -48,7 +48,7 @@ func validateRemovePodsViolatingInterPodAntiAffinityParams(params *api.StrategyP
 }
 
 // RemovePodsViolatingInterPodAntiAffinity evicts pods on the node which are having a pod affinity rules.
-func RemovePodsViolatingInterPodAntiAffinity(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor) {
+func RemovePodsViolatingInterPodAntiAffinity(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor, opts ...StrategyOption) {
 	if err := validateRemovePodsViolatingInterPodAntiAffinityParams(strategy.Params); err != nil {
 		klog.ErrorS(err, "Invalid RemovePodsViolatingInterPodAntiAffinity parameters")
 		return
diff --git a/pkg/descheduler/strategies/pod_lifetime.go b/pkg/descheduler/strategies/pod_lifetime.go
index 3720ea5e4..9a6de7175 100644
--- a/pkg/descheduler/strategies/pod_lifetime.go
+++ b/pkg/descheduler/strategies/pod_lifetime.go
@@ -56,7 +56,7 @@ func validatePodLifeTimeParams(params *api.StrategyParameters) error {
 }
 
 // PodLifeTime evicts pods on nodes that were created more than strategy.Params.MaxPodLifeTimeSeconds seconds ago.
-func PodLifeTime(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor) {
+func PodLifeTime(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor, opts ...StrategyOption) {
 	if err := validatePodLifeTimeParams(strategy.Params); err != nil {
 		klog.ErrorS(err, "Invalid PodLifeTime parameters")
 		return
diff --git a/pkg/descheduler/strategies/toomanyrestarts.go b/pkg/descheduler/strategies/toomanyrestarts.go
index 11dddc402..6a2b542e0 100644
--- a/pkg/descheduler/strategies/toomanyrestarts.go
+++ b/pkg/descheduler/strategies/toomanyrestarts.go
@@ -49,7 +49,7 @@ func validateRemovePodsHavingTooManyRestartsParams(params *api.StrategyParameter
 // RemovePodsHavingTooManyRestarts removes the pods that have too many restarts on node.
 // There are too many cases leading this issue: Volume mount failed, app error due to nodes' different settings.
 // As of now, this strategy won't evict daemonsets, mirror pods, critical pods and pods with local storages.
-func RemovePodsHavingTooManyRestarts(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor) {
+func RemovePodsHavingTooManyRestarts(ctx context.Context, client clientset.Interface, strategy api.DeschedulerStrategy, nodes []*v1.Node, podEvictor *evictions.PodEvictor, opts ...StrategyOption) {
 	if err := validateRemovePodsHavingTooManyRestartsParams(strategy.Params); err != nil {
 		klog.ErrorS(err, "Invalid RemovePodsHavingTooManyRestarts parameters")
 		return
diff --git a/pkg/descheduler/strategies/topologyspreadconstraint.go b/pkg/descheduler/strategies/topologyspreadconstraint.go
index 8dda686d2..6fc729865 100644
--- a/pkg/descheduler/strategies/topologyspreadconstraint.go
+++ b/pkg/descheduler/strategies/topologyspreadconstraint.go
@@ -54,6 +54,7 @@ func RemovePodsViolatingTopologySpreadConstraint(
 	strategy api.DeschedulerStrategy,
 	nodes []*v1.Node,
 	podEvictor *evictions.PodEvictor,
+	opts ...StrategyOption,
 ) {
 	strategyParams, err := validation.ValidateAndParseStrategyParams(ctx, client, strategy.Params)
 	if err != nil {
